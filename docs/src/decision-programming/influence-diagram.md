# Influence Diagram
## Introduction
Based on [^1], sections 3.

The paper [^2] explains details about influence diagrams.


## Definition
We define the **influence diagram** as a directed, acyclic graph

$$G=(C,D,V,I,S).$$

The nodes $N=CâˆªDâˆªV$ consists of **chance nodes** $C,$ **decision nodes** $D,$ and **value nodes** $V$. We index the chance and decision nodes such that $CâˆªD=\{1,...,n\}$ and values nodes such that $V=\{n+1,...,n+|V|\}$ where $n=|C|+|D|.$

We define the **information set** $I$ of node $jâˆˆN$ as

$$I(j)âŠ†\{iâˆˆCâˆªDâˆ£i<j\}$$

The condition enforces that the graph is directed and acyclic, and there are no arcs from value nodes to other nodes. Practically, the information set is an edge list to reverse direction in the graph.

We refer to $S$ as the **state space**. Each chance and decision node $jâˆˆCâˆªD$ is associates with a finite number of **states** $S_j$ that we encode using integers $\{1,...,|S_j|\}$ from one to number of states $|S_j|.$


## Root and Leaf Nodes
In the subdiagram of $G$ which consists of the chance and decision nodes $jâˆˆCâˆªD,$ we call node $j$ a **root** node if its information set if empty, that is, $I(j)=âˆ….$

Similarly, we call node $j$ a **leaf** node if it is not in any information set, that is, $jâˆ‰I(i)$ for all $iâˆˆCâˆªD.$ Each leaf node must be in at least one of the information sets of value nodes. That is, for each leaf node $j$ exists a value node $iâˆˆV$ such that $jâˆˆI(i).$ Otherwise, the node $j$ is **redundant**.


## Visualization
To visualize influence diagrams, we define the different node types and how to order the nodes. There are two ways to order directed acyclic graphs, linear and depth-wise. We use [diagrams.net](https://www.diagrams.net/) for drawing influence diagrams.

### Node Types
![](figures/node-types.svg)

We use a circle to represent chance nodes, square to represent decision nodes and diamond to represent value nodes. The symbol $i$ represents the node's index and symbol $S_i$ the states of the chance or decision node.

### Linear Order
![](figures/linear-order.svg)

We can order the nodes in increasing linear order based on indices.

### Depth-wise Order
![](figures/depth-wise-order.svg)

We define the **depth** of a node $jâˆˆN$ as follows. Root nodes have a depth of one

$$\operatorname{depth}(j)=1,\quad I(j)=âˆ….$$

Other nodes have a depth of one greater than the maximum depth of its predecessors

$$\operatorname{depth}(j)=\max_{iâˆˆI(j)} \operatorname{depth}(i) + 1,\quad I(j)â‰ âˆ….$$

We can group the nodes by their depth and then order them by increasing depth and increasing indices order within that depth. Compared to linear order, the depth-wise order is more concise. It displays more information about the influence relationships, because nodes can only be influenced by nodes with smaller depth.


## Paths
Paths in influence diagrams represent realizations of states for chance and decision nodes. Formally, a **path** is a sequence of states

$$ğ¬=(s_1, s_2, ...,s_n),$$

where each state $s_iâˆˆS_i$ for all chance and decision nodes $iâˆˆCâˆªD.$

We define a **subpath** of $ğ¬$ is a subsequence

$$(ğ¬_{i_1}, ğ¬_{i_2}, ..., ğ¬_{i_{k}}),$$

where $1â‰¤i_1<i_2<...<i_kâ‰¤n$ and $kâ‰¤n.$

The **information path** of node $jâˆˆN$ on path $ğ¬$ is a subpath defined as

$$ğ¬_{I(j)}=(ğ¬_i âˆ£ iâˆˆI(j)).$$

We define the set of **all paths** as a product set of all states

$$ğ’=âˆ_{jâˆˆCâˆªD} S_j.$$

The set of **information paths** of node $jâˆˆN$ is the product set of the states in its information set

$$ğ’_{I(j)}=âˆ_{iâˆˆI(j)} S_i.$$

We denote elements of the sets using notation $s_jâˆˆS_j$, $ğ¬âˆˆğ’$, and $ğ¬_{I(j)}âˆˆğ’_{I(j)}.$


## Probabilities
For each chance node $jâˆˆC$, we denote the **probability** of state $s_j$ given information path $ğ¬_{I(j)}$ as

$$â„™(X_j=s_jâˆ£X_{I(j)}=ğ¬_{I(j)})=â„™(s_jâˆ£ğ¬_{I(j)})âˆˆ[0, 1],$$

with

$$âˆ‘_{s_jâˆˆS_j} â„™(s_jâˆ£ğ¬_{I(j)}) = 1.$$

We refer to a chance state $s_jâˆˆS_j$ given information path $ğ¬_{I(j)}$ as **inactive** if its probability is zero $â„™(s_jâˆ£ğ¬_{I(j)})=0.$

Implementation wise, we can think probabilities as functions of information paths concatenated with state $X_j : ğ’_{I(j)};S_j â†’ [0, 1]$ where $âˆ‘_{s_jâˆˆS_j} X_j(ğ¬_{I(j)};s_j)=1.$


## Decision Strategy
For each decision node $jâˆˆD,$ a **local decision strategy** maps an information path $ğ¬_{I(j)}$ to a state $s_j$

$$Z_j:ğ’_{I(j)}â†¦S_j.$$

**Decision strategy** $Z$ contains one local decision strategy for each decision node. Set of **all decision strategies** is denoted $â„¤.$

A decision stategy $Zâˆˆâ„¤$ is **compatible** with the path $ğ¬âˆˆğ’$ if and only if $Z_j(ğ¬_{I(j)})=s_j$ forall $Z_jâˆˆZ$ and $jâˆˆD.$


## Path Probability
We define the **path probability (upper bound)** as

$$p(ğ¬) = âˆ_{jâˆˆC} â„™(ğ¬_jâˆ£ğ¬_{I(j)}).$$

The path probability $â„™(ğ¬âˆ£Z)$ equals $p(ğ¬)$ if the path $ğ¬$ is compatible with the decision strategy $Z$. Otherwise, the path cannot occur, and the probability is zero.


## Consequences
For each value node $jâˆˆV$, we define the **consequence** given information path $ğ¬_{I(j)}$ as

$$Y_j:ğ’_{I(j)}â†¦â„‚,$$

where $â„‚$ is the set of real-valued consequences.


## Path Utility
The **utility function** is a function that maps consequences to real-valued utility

$$U:â„‚^{|V|}â†¦â„.$$

The **path utility** is defined as the utility function acting on the consequences of value nodes given their information paths

$$\mathcal{U}(ğ¬) = U(\{Y_j(ğ¬_{I(j)}) âˆ£ jâˆˆV\}).$$

The **default path utility** is the sum of consequences

$$\mathcal{U}(ğ¬) = âˆ‘_{jâˆˆV} Y_j(ğ¬_{I(j)}).$$


## Path Distribution
A **path distribution** is a pair

$$(â„™(ğ¬âˆ£Z), \mathcal{U}(ğ¬))$$

that comprises of path probability function and path utility function over paths $ğ¬âˆˆğ’$ conditional to the decision strategy $Z.$


## Active Paths
An **active path** is a path $ğ¬âˆˆğ’$ that has positive path probability $â„™(ğ¬âˆ£Z)>0.$ We denote the set of **all active paths** given a decision strategy $Z$ as

$$ğ’(Z)=\{ğ¬âˆˆğ’ âˆ£ â„™(ğ¬âˆ£Z)>0\}.$$

Since each decision strategy $Z_j$ chooses only one of its states the **number of active paths** is bounded by

$$|ğ’(Z)|â‰¤|ğ’|/\prod_{jâˆˆD}|S_j|=\prod_{jâˆˆC}|S_j|.$$

If an influece diagram has **zero inactive chance states** the number of active paths is equal to the upper bound

$$|ğ’(Z)|=\prod_{jâˆˆC}|S_j|.$$


## Properties
In this section, we define common properties for influence diagrams. The paper [^2] discusses many of these properties.

**Discrete** influence diagram refers to countable state space. Otherwise, the influence diagram is **continuous**. We can discretize continuous influence diagrams using discrete bins.

Influence diagram is **symmetric** if there is zero inactive chance states. Otherwise, it is **assymetric**.

Two nodes are **sequential** if there exists a directed path from one node to the other in the influence diagram. Otherwise, the nodes are **parallel**. Sequential nodes often model time dimension.

**Repeated subdiagram** refers to a recurring pattern within an influence diagram. Often, influence diagrams do not have a unique structure, but they consist of a repeated pattern due to the underlying problem's properties.

**Limited-memory** influence diagram refers to an influence diagram where an upper bound limits the size of the information set for decision nodes. It is a desired attribute because it affects the decision model size, as discussed in the [Computational Complexity](@ref) section.

**Isolated subdiagrams** refer to an influence diagram that consists of multiple unconnected diagrams, that is, there are no undirected connections between the diagrams. Therefore, one isolated subdiagram's decisions affect decisions on the other isolated subdiagrams only through the utility function.


## References
[^1]: Salo, A., Andelmin, J., & Oliveira, F. (2019). Decision Programming for Multi-Stage Optimization under Uncertainty, 1â€“35. Retrieved from [http://arxiv.org/abs/1910.09196](http://arxiv.org/abs/1910.09196)

[^2]: Bielza, C., GÃ³mez, M., & Shenoy, P. P. (2011). A review of representation issues and modeling challenges with influence diagrams. Omega, 39(3), 227â€“241. https://doi.org/10.1016/j.omega.2010.07.003
